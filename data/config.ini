################################################################################
#               Setting for General Parameter for Searching
################################################################################

[search]

# display the results in the search process or not ( default: True )
is_disp = True

# determine the score function for bayesian optimization.
# TS, EI and PI are available ( default: TS )
score = TS

# maximum number of search
# ( default: 100 )
max_search = 100

# maximum number of initial random search. If the value is zero,
# random search is not performed ( you have to prepare the training data )
# ( default: 20 )
num_rand_search = 20

# directory name for saving the results ( default: res )
dir_name = res

# meta parameter which controls the amount of validation in TS. ( default: 1.0 )
alpha = 1.0

################################################################################
#               Setting for Prediction
################################################################################

[predict]
# employ the randomized kernel expansion or not ( default: False )
is_rand_expans = True

# number of basis functions ( default: 5000 )
num_basis = 5000



################################################################################
#               Setting for Hyper Parameter Learning
################################################################################

[learning]

# method for hyper parameter learning. adam ( online ) or bfgs ( batch )
# (default: adam)
# caution: The current BFGS implementation involves the critical numerical
# instability due to the Cholesky decomposition in the gradient computation step,
# thus it may be better to use adam
method = adam

# optimize hyper parameters or not ( default: False )
is_hyparams_learning = True

# display the results in the learning process or not (default: False )
is_disp = False

# number of displaying the learning results (default: 10 )
num_disp = 10

# COMBO can automatically adjust the initial parameters by repeating the shallow
# learning processes on trials.
# 'num_init_params_search' is the number of searching the initial hyper parameters.
# If setting the value to zero,
# the initial search for hyper parameters is not performed.
num_init_params_search = 20


# The number of interval to perform the hyper parameter learning.
# If it takes 0 and is_hyparams_learning = True,
# the hyper parameter learning is only performed before starting the bayesian search.
interval = 20


[online]
# maximum number of epochs (default: 2000)
max_epoch = 2000

# maximum number of epochs in the initial hyper parameter search phase (default: 20)
max_epoch_init_params_search = 20

# size of mini_batch. (default: 64)
batch_size = 64


# size of mini_batch for evaluating the goodness of current hyper parameters
# ( for showing the results of hyper parameter learnings )
# ( default: 5000 )
eval_size = 5000

[adam]
# meta parameters of adam. The details can be shown in [].
alpha = 0.001
beta = 0.9
gamma = 0.999
epsilon = 1e-6


[batch]
# COMBO can automatically adjust the initial parameters by repeating the shallow
# learning processes on trials.
# 'num_init_params_search' is the number of searching the initial hyper parameters.
# If setting this value to zero,
# the initial search for hyper parameters is not performed.
num_init_params_search = 20

# maximum number of BFGS iterations
max_iter = 300

# maximum number of BFGS iterations in the initial hyper parameter searching process.
max_iter_init_params_search = 20
